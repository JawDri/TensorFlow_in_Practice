In the last couple of weeks we looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave us a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. But sentiment can also be determined by the sequence in which words appear. For example, we could have 'not fun', which of course is the opposite of 'fun'. This week we'll start digging into a variety of model formats that are used in training models to understand context in sequence!
